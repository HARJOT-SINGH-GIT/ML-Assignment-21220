{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73746211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hmmlearn\n",
    "#pip install --upgrade hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20eadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmmlearn\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1569040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt  # show graph\n",
    "import random\n",
    "\n",
    "#some other libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c69072",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"NER.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b55f3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(method=\"ffill\")\n",
    "data = data.rename(columns={'Sentence #': 'sentence'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31adb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text_column):\n",
    "    # lowercase all text in the column\n",
    "    text_column = text_column.str.lower()\n",
    "\n",
    "    # replacing numbers with NUM token\n",
    "    text_column = text_column.str.replace(r'\\d+', 'NUM')\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_column = text_column.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    return text_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37dd2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_precessed = pre_processing(data.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d414470",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = data\n",
    "data_processed['Word'] = data_pre_precessed\n",
    "\n",
    "\n",
    "data_processed = data_processed[(data_processed['Word'] != '') | (data_processed['Word'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "270fae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 31683)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data.POS.values))  \n",
    "words = list(set(data.Word.values))  \n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "133aede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1 = list(set(data_processed.Word.values))  \n",
    "len(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ad8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f800246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence      Word  POS Tag\n",
       "24  Sentence: 2  families  NNS   O\n",
       "25  Sentence: 2             IN   O\n",
       "26  Sentence: 2  soldiers  NNS   O\n",
       "27  Sentence: 2    killed  VBN   O\n",
       "28  Sentence: 2             IN   O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a96f68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td></td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS Tag\n",
       "0  Sentence: 1      thousands  NNS   O\n",
       "1  Sentence: 1                  IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1                 VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7597ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = data_processed.POS\n",
    "X1 = data_processed.drop('POS', axis=1)\n",
    "data_processed.reset_index(drop=True, inplace=True)\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix1, test_ix1 = next(gs.split(X1, y1, groups=data_processed['sentence']))\n",
    "\n",
    "data_train1 = data_processed.loc[train_ix1]\n",
    "data_test1 = data_processed.loc[test_ix1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708b2c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence      Word  POS Tag\n",
       "13  Sentence: 2  families  NNS   O\n",
       "14  Sentence: 2  soldiers  NNS   O\n",
       "15  Sentence: 2    killed  VBN   O\n",
       "16  Sentence: 2  conflict   NN   O\n",
       "17  Sentence: 2    joined  VBD   O"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6815f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS    Tag\n",
       "0  Sentence: 1      thousands  NNS      O\n",
       "1  Sentence: 1  demonstrators  NNS      O\n",
       "2  Sentence: 1        marched  VBN      O\n",
       "3  Sentence: 1         london  NNP  B-geo\n",
       "4  Sentence: 1        protest   VB      O"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a97d4ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 24969)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN'\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0446defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702936/702936 [00:00<00:00, 1152308.81it/s]\n"
     ]
    }
   ],
   "source": [
    "count_tags = dict(data_train.POS.value_counts()) \n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
    "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4bd2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 82.37it/s]\n"
     ]
    }
   ],
   "source": [
    "startprob = np.zeros((len(tags),))\n",
    "transmat = np.zeros((len(tags), len(tags)))\n",
    "emissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e800e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24970, 24970)\n"
     ]
    }
   ],
   "source": [
    "count_words = {}\n",
    "for word in data_train.Word.values:\n",
    "    count_words[word] = count_words.get(word, 0) + 1\n",
    "\n",
    "\n",
    "count_word_transitions = {}\n",
    "for sentence in data_train.groupby('sentence'):\n",
    "    words = sentence[1]['Word'].values\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i+1]\n",
    "        if w1 not in count_word_transitions:\n",
    "            count_word_transitions[w1] = {}\n",
    "        count_word_transitions[w1][w2] = count_word_transitions[w1].get(w2, 0) + 1\n",
    "\n",
    "        \n",
    "word_transition_matrix = np.zeros((len(word2id)+1, len(word2id)+1))\n",
    "sum_words_to_next_words = np.sum([count_word_transitions[w1][w2] for w1 in count_word_transitions for w2 in count_word_transitions[w1]])\n",
    "for w1, w1id in word2id.items():\n",
    "    for w2, w2id in word2id.items():\n",
    "        word_transition_matrix[w1id][w2id] = count_word_transitions.get(w1, {}).get(w2, 0) / sum_words_to_next_words\n",
    "print(word_transition_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39a8a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(sentence: List[str], word_transition_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Given a sentence and word_transition_matrix, returns the log-likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    # converting the sentence to a list of word IDs\n",
    "    sentence_ids = [word2id.get(w, word2id['UNKNOWN']) for w in sentence]\n",
    "\n",
    "    # calculating the log-likelihood using the word transition matrix\n",
    "    log_likelihood = np.log(word_transition_matrix[sentence_ids[0]][sentence_ids[1]])\n",
    "    for i in range(1, len(sentence_ids) - 1):\n",
    "        log_likelihood += np.log(word_transition_matrix[sentence_ids[i]][sentence_ids[i+1]] + 1e-10)\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6971fb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.259970813020175"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_log_likelihood([\"This\", \"is\", \"a\", \"test\", \"sentence\"], word_transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a5cd670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    }
   ],
   "source": [
    "# model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "# model.startprob_ = startprob\n",
    "# model.transmat_ = transmat\n",
    "# model.emissionprob_ = emissionprob\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "\n",
    "# Set the model parameters\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0193cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345639/345639 [00:00<00:00, 2752397.03it/s]\n"
     ]
    }
   ],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "\n",
    "\n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9f63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41994ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a55eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array:\n",
    "    \"\"\"\n",
    "     Write the viterbi algorithm from scratch to find the best probable path\n",
    "     attr:\n",
    "       pi: initial probabilities\n",
    "       a: transition probabilities\n",
    "       b: emission probabilities\n",
    "       obs: list of observations\n",
    "     return:\n",
    "       array of the indices of the best hidden states\n",
    "    \"\"\"\n",
    "    # state space cardinality\n",
    "    K = a.shape[0]\n",
    "\n",
    "    # observation sequence length\n",
    "    T = len(obs)\n",
    "\n",
    "    # initializing the tracking tables from first observation\n",
    "    delta = np.zeros((T, K))\n",
    "    psi = np.zeros((T, K))\n",
    "    delta[0] = pi * b[:, obs[0]]\n",
    "\n",
    "    # iterating throught the observations updating the tracking tables\n",
    "    for t in range(1, T):\n",
    "        for j in range(K):\n",
    "            delta[t, j] = np.max(delta[t-1] * a[:, j] * b[j, obs[t]])\n",
    "            psi[t, j] = np.argmax(delta[t-1] * a[:, j])\n",
    "\n",
    "    # build the output, optimal model trajectory\n",
    "    x = np.zeros(T, dtype=int)\n",
    "    x[T-1] = np.argmax(delta[T-1])\n",
    "    for t in range(T-2, -1, -1):\n",
    "        x[t] = psi[t+1, x[t+1]]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12580e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hidden_states = ['Sunny', 'Cloudy', 'Rainy']\n",
    "observable_states = ['Hot', 'Mild', 'Cold', 'Windy', 'Foggy']\n",
    "observations =  []\n",
    "\n",
    "for i in range(40):\n",
    "  obs_index = random.randint(0, len(observable_states)-1) \n",
    "  observations.append(obs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72caff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [3, 3, 0, 1, 1, 1, 4, 2, 1, 1, 2, 4, 2, 4, 0, 4, 0, 0, 3, 1, 0, 1, 4, 3, 3, 1, 1, 2, 4, 3, 3, 3, 0, 3, 2, 3, 1, 0, 3, 2]\n",
      "Viterbi sequence: [21 21 17 21 21 21 26  1 21 21  1 26  1 26  7 26 17 14 21 21 17 21 26 21\n",
      " 21 21 21  1 26 21 21 21 17 21  1 21 21 17 21  1]\n"
     ]
    }
   ],
   "source": [
    "hidden_state_sequence = viterbi(startprob, transmat, emissionprob, observations)\n",
    "\n",
    "print(\"Observations:\", observations)\n",
    "print(\"Viterbi sequence:\", hidden_state_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa98f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 442.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def baum_welch(observations, observations_vocab, n_hidden_states):\n",
    "    \"\"\"\n",
    "    Baum-Welch algorithm for estimating the HMM parameters\n",
    "    :param observations: observations\n",
    "    :param observations_vocab: observations vocabulary\n",
    "    :param n_hidden_states: number of hidden states to estimate\n",
    "    :return: a, b (transition matrix and emission matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        forward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined alpha_\n",
    "        \"\"\"\n",
    "        a_start = 1 / n_hidden_states\n",
    "        alpha_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        alpha_[:, 0] = a_start\n",
    "        for t in range(1, len(observations)):\n",
    "          for j in range(n_hidden_states):\n",
    "            calc = observations_vocab == observations[t]\n",
    "            for i in range(n_hidden_states):\n",
    "              alpha_[j, t] = sum(alpha_[i, t-1]*a_[i,j] * b_[j, np.where(calc)[0][0]] for i in range(n_hidden_states))\n",
    "\n",
    "        return alpha_\n",
    "\n",
    "    def backward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        backward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined beta_\n",
    "        \"\"\"\n",
    "        beta_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        beta_[:, -1:] = 1\n",
    "        for t in range(len(observations) -2, -1, -1):\n",
    "          for i in range(n_hidden_states):\n",
    "            calc2 = observations_vocab == observations[t+1]\n",
    "            beta_[i,t] = sum(a_[i,j] * b_[j, np.where(calc2)[0][0]]*beta_[j, t+1] for j in range(n_hidden_states))\n",
    "        return beta_\n",
    "\n",
    "    def compute_gamma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # gamma_prob = np.zeros(n_samples, len(observations))\n",
    "        gamma_prob = np.multiply(alfa, beta) / sum(np.multiply(alfa, beta))\n",
    "        return gamma_prob\n",
    "\n",
    "    def compute_sigma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sigma_prob = np.zeros((n_samples, len(observations) - 1, n_samples), dtype=float)\n",
    "        denomenator = np.multiply(alfa, beta)\n",
    "        for i in range(len(observations) - 1):\n",
    "            for j in range(n_samples):\n",
    "                for k in range(n_samples):\n",
    "                    index_in_vocab = np.where(vocab == observations[i + 1])[0][0]\n",
    "                    sigma_prob[j, i, k] = (alfa[j, i] * beta[k, i + 1] * a_[j, k] * b_[k, index_in_vocab]) / sum(\n",
    "                        denomenator[:, j])\n",
    "        return sigma_prob\n",
    "\n",
    "    # initialize A ,B\n",
    "    a = np.ones((n_hidden_states, n_hidden_states)) / n_hidden_states\n",
    "    b = np.ones((n_hidden_states, len(observations_vocab))) / len(observations_vocab)\n",
    "    for iter in tqdm(range(2000), position=0, leave=True):\n",
    "\n",
    "        # E-step caclculating sigma and gamma\n",
    "        alfa_prob = forward_probs(observations, observations_vocab, n_hidden_states, a, b)  #\n",
    "        beta_prob = backward_probs(observations, observations_vocab, n_hidden_states, a, b)  # , beta_val\n",
    "        gamma_prob = compute_gamma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "        sigma_prob = compute_sigma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "\n",
    "        # M-step caclculating A, B matrices\n",
    "        a_model = np.zeros((n_hidden_states, n_hidden_states))\n",
    "        for j in range(n_hidden_states):  # calculate A-model\n",
    "            for i in range(n_hidden_states):\n",
    "                for t in range(len(observations) - 1):\n",
    "                    a_model[j, i] = a_model[j, i] + sigma_prob[j, t, i]\n",
    "                normalize_a = [sigma_prob[j, t_current, i_current] for t_current in range(len(observations) - 1) for\n",
    "                               i_current in range(n_hidden_states)]\n",
    "                normalize_a = sum(normalize_a)\n",
    "                if normalize_a == 0:\n",
    "                    a_model[j, i] = 0\n",
    "                else:\n",
    "                    a_model[j, i] = a_model[j, i] / normalize_a\n",
    "\n",
    "        b_model = np.zeros((n_hidden_states, len(observations_vocab)))\n",
    "\n",
    "        for j in range(n_hidden_states):\n",
    "            for i in range(len(observations_vocab)):\n",
    "                indices = [idx for idx, val in enumerate(observations) if val == observations_vocab[i]]\n",
    "                numerator_b = sum(gamma_prob[j, indices])\n",
    "                denominator_b = sum(gamma_prob[j, :])\n",
    "                if denominator_b == 0:\n",
    "                    b_model[j, i] = 0\n",
    "                else:\n",
    "                    b_model[j, i] = numerator_b / denominator_b\n",
    "\n",
    "        a = a_model\n",
    "        b = b_model\n",
    "    return a, b\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observable_map = {'sleeping': 0, 'eating': 1, 'pooping': 2}\n",
    "observations = []\n",
    "for i in range(40):\n",
    "    observations.append(observable_map[random.choice(observable_states)])\n",
    "\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
    "                  n_hidden_states=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92806d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [1, 2, 2, 0, 2, 2, 1, 0, 2, 2, 1, 1, 0, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 2, 1, 0, 1, 0, 1, 1, 2, 2, 0, 1, 0, 2, 1, 0]\n",
      "Viterbi sequence: [21  1  1 17  1  1 21 17  1  1 21 21 17 21 17  1 17  1 17 14  1 17  1  1\n",
      " 21  1 21 17 21 17 21 21  1  1 17 21 17  1 21 17]\n"
     ]
    }
   ],
   "source": [
    "hidden_state_sequence = viterbi(startprob, transmat, emissionprob, observations)\n",
    "\n",
    "print(\"Observations:\", observations)\n",
    "print(\"Viterbi sequence:\", hidden_state_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
